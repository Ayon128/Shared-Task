{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Dataset/tamil_train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"../Dataset/tamil_test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>மாட்ட கடிச்சு ஆட்ட கடிச்சு அடுத்து கோழிய கடிச்சு.என்ன செய்யப்போறானுக? நாங்கள் என்ன சாப்பிடனும் சாப்பிடக்கூடாதுன்னு சொல்ல நீங்க யாருடா?\\nமக்களின்_சின்னம்_மைக்\\nமக்களின்_சின்னம்_ஒலிவாங்கி</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>₹752 கோடி போச்சா சோனமுத்தா\\n\\nசோனியா - ராகுல்காந்தி கதறல் \\n\\ncongressfails விடியாஅரசு dmkfailstn dmkfails rejectdmk திமுக</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>‘கை’ சின்னத்திற்கு ஆதரவு கொடுக்கும் ‘நம்ம ஊரு இந்தியன் தாத்தா’…\\n\\nvote4india rahulgandhi priyankagandhivadra mkstalin dmkitwing vote4sudha indiaallaince vendammodi novoteforbjp \\n\\n |  |  |  |</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>naaready ft edappadi version\\nமொத்தமா செதச்சு விட்டானுங்க  எப்பாசாமி யாருப்ப அந்த creater எனக்கே அவர பாக்கனும் போல இருக்கே. \\n\\nleo edappadipalanisamy modi admk politics\\n\\n</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>மத்திய சென்னையை தட்டி தூக்க தயாராகும் மருத்துவர் கார்த்திகேயன் \\n\\nசீமான்_சின்னம்_ஒலிவாங்கி</td>\n",
       "      <td>Opinionated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  content  \\\n",
       "172   மாட்ட கடிச்சு ஆட்ட கடிச்சு அடுத்து கோழிய கடிச்சு.என்ன செய்யப்போறானுக? நாங்கள் என்ன சாப்பிடனும் சாப்பிடக்கூடாதுன்னு சொல்ல நீங்க யாருடா?\\nமக்களின்_சின்னம்_மைக்\\nமக்களின்_சின்னம்_ஒலிவாங்கி             \n",
       "1431  ₹752 கோடி போச்சா சோனமுத்தா\\n\\nசோனியா - ராகுல்காந்தி கதறல் \\n\\ncongressfails விடியாஅரசு dmkfailstn dmkfails rejectdmk திமுக                                                                            \n",
       "2838  ‘கை’ சின்னத்திற்கு ஆதரவு கொடுக்கும் ‘நம்ம ஊரு இந்தியன் தாத்தா’…\\n\\nvote4india rahulgandhi priyankagandhivadra mkstalin dmkitwing vote4sudha indiaallaince vendammodi novoteforbjp \\n\\n |  |  |  |     \n",
       "1027  naaready ft edappadi version\\nமொத்தமா செதச்சு விட்டானுங்க  எப்பாசாமி யாருப்ப அந்த creater எனக்கே அவர பாக்கனும் போல இருக்கே. \\n\\nleo edappadipalanisamy modi admk politics\\n\\n                         \n",
       "949   மத்திய சென்னையை தட்டி தூக்க தயாராகும் மருத்துவர் கார்த்திகேயன் \\n\\nசீமான்_சின்னம்_ஒலிவாங்கி                                                                                                           \n",
       "\n",
       "           labels  \n",
       "172   Neutral      \n",
       "1431  Sarcastic    \n",
       "2838  Positive     \n",
       "1027  Sarcastic    \n",
       "949   Opinionated  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 0):\n",
    "    display(train_df.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "Negative              406\n",
       "Neutral               637\n",
       "None of the above     171\n",
       "Opinionated          1361\n",
       "Positive              575\n",
       "Sarcastic             790\n",
       "Substantiated         412\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"labels\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0     70\n",
       "1     51\n",
       "2    171\n",
       "3     75\n",
       "4    106\n",
       "5     46\n",
       "6     25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby(\"labels\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling = {\n",
    "    'Neutral': 0,\n",
    "    'Substantiated': 1,\n",
    "    'Opinionated': 2,\n",
    "    'Positive' : 3,\n",
    "    'Sarcastic': 4,\n",
    "    'Negative': 5,\n",
    "    'None of the above': 6\n",
    "}\n",
    "train_df['labels'] = train_df['labels'].apply(lambda x : labeling[x])\n",
    "test_df['labels'] = test_df['labels'].apply(lambda x : labeling[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Set the target number of instances for minority classes\n",
    "target_minority_count = 200\n",
    "\n",
    "for minority_class in [0,1,3,5,6]:\n",
    "    minority_indices = train_df[train_df['labels'] == minority_class].index\n",
    "    minority_data = train_df.loc[minority_indices]\n",
    "    \n",
    "    minority_oversampled = resample(\n",
    "        minority_data,\n",
    "        replace=True,\n",
    "        n_samples=target_minority_count,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_df = pd.concat([train_df, minority_oversampled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0     837\n",
       "1     612\n",
       "2    1361\n",
       "3     775\n",
       "4     790\n",
       "5     606\n",
       "6     371\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['content']\n",
    "y_train = train_df['labels']\n",
    "X_test = test_df['content']\n",
    "y_test = test_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer().fit(X_train)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "y_pred = lr.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.20      0.21        70\n",
      "           1       0.10      0.06      0.07        51\n",
      "           2       0.41      0.68      0.51       171\n",
      "           3       0.37      0.29      0.33        75\n",
      "           4       0.41      0.25      0.31       106\n",
      "           5       0.15      0.09      0.11        46\n",
      "           6       0.85      0.68      0.76        25\n",
      "\n",
      "    accuracy                           0.37       544\n",
      "   macro avg       0.36      0.32      0.33       544\n",
      "weighted avg       0.35      0.37      0.35       544\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 14   6  35   3   8   3   1]\n",
      " [ 10   3  26   6   3   3   0]\n",
      " [  8   9 116  14  19   4   1]\n",
      " [ 12   5  24  22   6   5   1]\n",
      " [  9   6  48   9  27   7   0]\n",
      " [  8   0  28   5   1   4   0]\n",
      " [  2   0   3   1   2   0  17]]\n",
      "Macro F1 Score: 0.3294978305150858\n",
      "Macro Precision: 0.35993713456767645\n",
      "Macro Recall: 0.32174184838795983\n",
      "Accuracy: 0.37316176470588236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print metrics\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_tfidf, y_train)\n",
    "y_pred = svc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.23      0.23        70\n",
      "           1       0.11      0.08      0.09        51\n",
      "           2       0.41      0.65      0.50       171\n",
      "           3       0.31      0.27      0.29        75\n",
      "           4       0.43      0.25      0.31       106\n",
      "           5       0.19      0.09      0.12        46\n",
      "           6       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.37       544\n",
      "   macro avg       0.36      0.33      0.33       544\n",
      "weighted avg       0.35      0.37      0.34       544\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 16   7  31   4   5   6   1]\n",
      " [ 11   4  26   7   2   1   0]\n",
      " [ 11  12 111  16  19   1   1]\n",
      " [ 15   4  28  20   5   3   0]\n",
      " [ 11   8  45   9  26   6   1]\n",
      " [  6   0  26   8   2   4   0]\n",
      " [  2   0   3   1   1   0  18]]\n",
      "Macro F1 Score: 0.3318222439319295\n",
      "Macro Precision: 0.36232339089481946\n",
      "Macro Recall: 0.3250045450588162\n",
      "Accuracy: 0.36580882352941174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print metrics\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    bootstrap=False,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.13      0.17        70\n",
      "           1       0.33      0.10      0.15        51\n",
      "           2       0.39      0.82      0.53       171\n",
      "           3       0.42      0.24      0.31        75\n",
      "           4       0.44      0.22      0.29       106\n",
      "           5       0.13      0.04      0.07        46\n",
      "           6       0.70      0.76      0.73        25\n",
      "\n",
      "    accuracy                           0.40       544\n",
      "   macro avg       0.38      0.33      0.32       544\n",
      "weighted avg       0.38      0.40      0.34       544\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  9   1  48   5   4   2   1]\n",
      " [  6   5  30   3   4   3   0]\n",
      " [  3   2 140   7  12   3   4]\n",
      " [  5   4  38  18   7   2   1]\n",
      " [  4   3  66   6  23   3   1]\n",
      " [  5   0  33   4   1   2   1]\n",
      " [  3   0   2   0   1   0  19]]\n",
      "Macro F1 Score: 0.3208305343966898\n",
      "Macro Precision: 0.3829403476755441\n",
      "Macro Recall: 0.32939764107073394\n",
      "Accuracy: 0.39705882352941174\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print metrics\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "y_pred = xgb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.06      0.07        70\n",
      "           1       0.14      0.06      0.08        51\n",
      "           2       0.38      0.67      0.48       171\n",
      "           3       0.31      0.27      0.29        75\n",
      "           4       0.42      0.21      0.28       106\n",
      "           5       0.15      0.07      0.09        46\n",
      "           6       0.51      0.80      0.62        25\n",
      "\n",
      "    accuracy                           0.34       544\n",
      "   macro avg       0.29      0.30      0.27       544\n",
      "weighted avg       0.30      0.34      0.30       544\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  4   3  40   7   7   4   5]\n",
      " [  5   3  28  11   3   1   0]\n",
      " [  8   4 115  15  16   7   6]\n",
      " [ 10   6  35  20   1   2   1]\n",
      " [  9   5  53   9  22   3   5]\n",
      " [  4   1  30   2   4   3   2]\n",
      " [  0   0   5   0   0   0  20]]\n",
      "Macro F1 Score: 0.27392968312577415\n",
      "Macro Precision: 0.2860850688958347\n",
      "Macro Recall: 0.30398746203142835\n",
      "Accuracy: 0.34375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print metrics\n",
    "print(\"Macro F1 Score:\", macro_f1)\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
